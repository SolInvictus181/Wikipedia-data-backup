import requests
from bs4 import BeautifulSoup
from docx import Document

# Define the URL to start scraping from (main page of English Wikipedia)
url = 'https://en.wikipedia.org/wiki/Main_Page'

# Initialize a document object for Word output
document = Document()

# Function to scrape the content of a single page and add it to the Word document
def scrape_page(url, depth=0, max_depth=2):
    if depth > max_depth:
        return

    try:
        # Send a GET request to the URL and retrieve the HTML content
        response = requests.get(url)
        response.raise_for_status()  # Raise an error for bad responses
        html_content = response.text

        # Use BeautifulSoup to parse the HTML content
        soup = BeautifulSoup(html_content, 'html.parser')

        # Extract the page title and add it to the Word document as a heading
        page_title = soup.find('h1', {'id': 'firstHeading'}).text
        document.add_heading(page_title, level=min(depth + 1, 6))  # Limit maximum heading level

        # Extract the page content and add it to the Word document as a paragraph
        page_content = soup.find('div', {'class': 'mw-parser-output'}).text
        document.add_paragraph(page_content)

        # Recursively scrape linked pages on the page and add their content to the Word document
        for link in soup.find_all('a', href=True):
            if link['href'].startswith('/wiki/') and ':' not in link['href']:
                linked_page_url = 'https://en.wikipedia.org' + link['href']
                scrape_page(linked_page_url, depth + 1, max_depth)

    except requests.exceptions.RequestException as e:
        print(f"Error scraping {url}: {e}")

# Call the scrape_page function to start scraping from the main page
scrape_page(url)

# Save the Word document as a file
document.save('wikipedia_scraped.docx')

